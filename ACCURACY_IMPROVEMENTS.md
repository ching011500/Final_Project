# LLM 問答準確度提升方案

## 📊 已實施的改進

### 1. 診斷日誌系統 ✅
- **位置**: `llm_query.py`
- **功能**: 追蹤關鍵步驟的執行情況
- **追蹤點**:
  - 查詢開始（參數提取）
  - RAG 檢索完成（檢索結果數量和相似度分數）
  - 過濾完成（過濾前後數量對比）
  - Context 建立（context 長度和預覽）
  - LLM 回答生成（回答長度和預覽）
  - 回答驗證（驗證分數和警告）
  - 重新排序（排序分數）

**日誌路徑**: `.cursor/debug.log` (NDJSON 格式)

### 2. LLM 回答驗證機制 ✅
- **位置**: `llm_query.py::_validate_answer()`
- **功能**: 驗證 LLM 回答是否與檢索到的課程資料一致
- **驗證項目**:
  - 課程名稱覆蓋率檢查
  - 課程代碼覆蓋率檢查
  - 編造資訊檢測（回答中提到的課程是否在檢索結果中）
  - 遺漏課程檢測（回答是否遺漏了相關課程）

**驗證分數計算**:
- 課程名稱覆蓋率 × 0.6 + 課程代碼覆蓋率 × 0.4
- 如果分數 < 0.3，標記為無效並添加警告

### 3. 重新排序機制 ✅
- **位置**: `llm_query.py::_rerank_courses()`
- **功能**: 根據查詢相關性重新排序檢索結果
- **排序因子**:
  - 原始相似度分數（基礎分數）
  - 關鍵詞匹配度（+0.2）
  - 年級匹配（+0.3 精確匹配，+0.15 部分匹配）
  - 必選修匹配（+0.2）
  - 系所匹配（+0.15）

### 4. Prompt 優化 ✅
- **改進點**:
  - 添加更嚴格的規則要求 LLM 檢查每個提到的課程是否在資料中
  - 明確要求如果沒有符合條件的課程，要明確告知使用者
  - 加強對編造資訊的警告

## 🔍 進一步改進建議

### 1. 實現結構化輸出（JSON Mode）
**問題**: 當前 LLM 輸出是自由文本，難以驗證和解析

**解決方案**:
```python
# 在 LLM 調用時使用 JSON mode
response = self.openai_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[...],
    response_format={"type": "json_object"},  # 啟用 JSON mode
    ...
)
```

**優點**:
- 更容易驗證回答的正確性
- 可以提取結構化資訊進行進一步處理
- 減少 LLM 編造資訊的可能性

### 2. 實現多階段驗證
**問題**: 當前驗證機制較簡單，可能無法捕捉所有錯誤

**解決方案**:
- **階段 1**: 檢查回答中提到的課程是否在檢索結果中
- **階段 2**: 使用 LLM 進行事實檢查（讓另一個 LLM 驗證回答）
- **階段 3**: 如果驗證失敗，重新生成回答或添加明確警告

### 3. 實現查詢擴展（Query Expansion）
**問題**: 使用者查詢可能不夠精確，導致檢索結果不理想

**解決方案**:
```python
def expand_query(self, query: str) -> List[str]:
    """使用 LLM 擴展查詢，生成同義詞和相關查詢"""
    # 例如：「資工系必修」→「資訊工程系必修」、「CS 必修」等
```

### 4. 實現結果多樣性控制
**問題**: 檢索結果可能過於相似，缺乏多樣性

**解決方案**:
- 使用 MMR (Maximal Marginal Relevance) 算法
- 確保返回的課程涵蓋不同的系所、年級、類型

### 5. 實現回答質量評分
**問題**: 無法量化回答的質量

**解決方案**:
```python
def score_answer_quality(self, answer: str, courses: List[Dict], query: str) -> float:
    """評分回答質量（0-1）"""
    scores = {
        'completeness': ...,  # 完整性
        'accuracy': ...,        # 準確性
        'relevance': ...,       # 相關性
        'clarity': ...          # 清晰度
    }
    return weighted_average(scores)
```

### 6. 實現錯誤回退機制
**問題**: 當 LLM 回答不準確時，沒有回退方案

**解決方案**:
- 如果驗證失敗，嘗試使用模板生成回答
- 或者返回結構化的課程列表，而不是自由文本

### 7. 優化混合檢索權重
**問題**: 當前 BM25 和 Embedding 的權重是固定的（0.4 和 0.6）

**解決方案**:
- 根據查詢類型動態調整權重
- 例如：精確查詢（如課程代碼）更依賴 BM25，語義查詢更依賴 Embedding

### 8. 實現快取機制
**問題**: 相同查詢會重複調用 LLM，浪費資源

**解決方案**:
- 快取常見查詢的結果
- 使用查詢的 hash 作為 key

### 9. 實現 A/B 測試框架
**問題**: 無法比較不同改進方案的效果

**解決方案**:
- 記錄每次查詢的結果和驗證分數
- 比較不同 prompt、模型、參數的效果

### 10. 加強錯誤處理
**問題**: 當前錯誤處理較簡單

**解決方案**:
- 分類錯誤類型（檢索錯誤、LLM 錯誤、驗證錯誤等）
- 針對不同錯誤類型提供不同的處理策略

## 📈 監控指標

建議追蹤以下指標來評估改進效果：

1. **檢索準確率**: 檢索到的課程中符合條件的比例
2. **回答驗證分數**: 平均驗證分數
3. **編造資訊率**: 回答中提到但不在檢索結果中的課程比例
4. **遺漏課程率**: 應該提到但未提到的課程比例
5. **使用者滿意度**: 如果有的話

## 🧪 測試建議

1. **單元測試**: 測試驗證機制、重新排序等函數
2. **整合測試**: 測試完整的查詢流程
3. **回歸測試**: 使用 `test_query.py` 中的測試查詢
4. **壓力測試**: 測試大量查詢的穩定性

## 📝 使用診斷日誌

日誌文件位於 `.cursor/debug.log`，格式為 NDJSON（每行一個 JSON 對象）。

**查看日誌**:
```bash
# 查看所有日誌
cat .cursor/debug.log | jq .

# 查看特定假設的日誌
cat .cursor/debug.log | jq 'select(.hypothesisId == "A")'

# 查看驗證相關的日誌
cat .cursor/debug.log | jq 'select(.message == "Answer validation")'
```

**分析建議**:
1. 檢查 RAG 檢索的相似度分數是否合理
2. 檢查過濾後是否保留了足夠的課程
3. 檢查驗證分數，如果太低需要優化 prompt
4. 檢查重新排序是否提升了相關性

